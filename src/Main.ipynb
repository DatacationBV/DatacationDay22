{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Datacation Day 2022 - Data Science use case </h1>\n",
    "<hr>\n",
    "\n",
    "<p> \n",
    "    Welcome to the code deck used for the Datacation Day 2022. <br>\n",
    "    You will be working with a real-world data set, that is collected by a digital auction house. <br>\n",
    "    The data is enclosed within a SQLite database and contains three related tables: <i>Auctions, Lots</i> and <i>Bids</i>. <br>\n",
    "    Auctions concern the actual events at which lots (items) are auctioned, after which the bids table contains all bids placed. <br>\n",
    "    A more detailed overview of what these tables are comprised of is given below:\n",
    "    <ol>\n",
    "    <li> \n",
    "        <b><u>auctions</u></b> \n",
    "        <ul>\n",
    "            <li> <b>id:</b> the auction id, uniquely identifying an auction.  </li>\n",
    "            <li> <b>relatedCompany:</b> the concerning company for which the items will be auctioned. </li>\n",
    "            <li> <b>auctionStart:</b> the date and time at which the auction started. </li>\n",
    "            <li> <b>auctionEnd:</b> the date and time at which the auction ended. </li>\n",
    "            <li> <b>branchCategory:</b> the branch to which the product to be auctioned are categorized. </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b><u>lots</u></b>\n",
    "        <ul>\n",
    "            <li> <b>countryCode:</b> description of the country the lot is auctioned in. </li>\n",
    "            <li> <b>saleDate:</b> the date and time at which the lot is sold. </li>\n",
    "            <li> <b>auctionID:</b> the id reference of the auction at which the lot is offered. </li>\n",
    "            <li> <b>lotNr:</b> the numeric indicator of the lot within its auction. </li>\n",
    "            <li> <b>suffix:</b> additional information to the lot number. </li>\n",
    "            <li> <b>numberOfItems:</b> the number of items offered within the lot. </li>\n",
    "            <li> <b>buyerAccountID:</b> the id of the bidder who won the auction and bought the lot. </li>\n",
    "            <li> <b>estimatedValue:</b> the estimated value of the items comprising the lot. </li>\n",
    "            <li> <b>StartingBid:</b> the initial price for which the lot is offered. </li>\n",
    "            <li> <b>reserveBid:</b> the minimum amount that the seller will accept as the winning bid. </li>\n",
    "            <li> <b>currentBid:</b> the actual bid offered for the auctioned lot. </li>\n",
    "            <li> <b>vat:</b> the percentage tax payed for the auctioned lot. </li>\n",
    "            <li> <b>category:</b> the category of products to which this lot is assigned. </li>\n",
    "            <li> <b>sold:</b> indicator whether the lot is sold or is left unsold. </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b><u>bids</b></u>\n",
    "        <ul>\n",
    "            <li> <b>auctionID:</b> the id reference of the auction at which the lot is offered. </li>\n",
    "            <li> <b>lotNr:</b> the numeric indicator of the lot in which the bid is made. </li>\n",
    "            <li> <b>lotID:</b> reference ID describing the lot in which the bid is made. </li>\n",
    "            <li> <b>isCombination:</b> indicator if the bid is considered within a combination of bids. </li>\n",
    "            <li> <b>accountID:</b> the id of the bidder who placed the bid. </li>\n",
    "            <li> <b>isCompany:</b> indicator whether the bidder concerns a company. </li>\n",
    "            <li> <b>bidPrice:</b> the price the bidder offered. </li>\n",
    "            <li> <b>biddingDateTime:</b> the time the bid was placed by the bidder. </li>\n",
    "            <li> <b>closingDateTime:</b> the time the lot is planned to close. </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    </ol>\n",
    "    As a Data Professional, your task is to use this data to improve the process of the auction house. <br>\n",
    "    Improvements can be made on several areas, of which we give you some indication of what is possible:\n",
    "    <ol>\n",
    "        <li>Clustering of different Bidding Behavior.</li>\n",
    "        <li>Prediction of sale or no sale.</li>\n",
    "        <li>Automatically setting an \"optimal\" starting bid.</li>\n",
    "        <li> ... </li>\n",
    "    </ol>\n",
    "    The three subjects mentioned above are solemnly given as a guideline for which we will also give you some boilerplate code to get started. <br>\n",
    "    However useful, the given subjects are only examples, as you are free to explore your own solution direction. <br>\n",
    "    Of course creativity is rewarded, as consultancy often asks for innovative solutions that your client did not think of before. <br>\n",
    "    At the bottom of the notebook some room is reserved for your own ideas, space in which you can let your imagination run freely! <br>\n",
    "    <br>\n",
    "    We look forward to what insights you will present at the end of the day for the board of the Auction house... which is us! :-).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3> A: Setting up the (virtual) environment Import libraries and connect to the Database object </h3>\n",
    "\n",
    "<p>\n",
    "To allow the code to run, we have to create a virtual environment containing all libraries that we will use. <br>\n",
    "We prepared a <i>requirements.txt</i> file that can be used to simply download all these libraries, following the steps given below. <br>\n",
    "\n",
    "<ol>\n",
    "    <li> Open the <b>command prompt</b> pressing the two keys: 'CTRL' + '~' simultaneously. </li>\n",
    "    <li> Create a <b>virtual environment</b> using the code: <code>python -m venv venv</code> in your command prompt. </li>\n",
    "    <li> If not done automatically, <b>activate</b> the virtual environment using the code: <code> .\\venv\\Scripts\\activate </code> in your command prompt. </li>\n",
    "    <li> Check if the command prompt shows <b>(venv)</b> in green in front of the input line. </li>\n",
    "    <li> The final step is to <b>install all libraries</b> needed, doing this can be done using the code: <code>pip install -r requirements.txt</code> </li>\n",
    "    <li> Ensure that the notebook will be run using your just created venv by selecting the venv at the top right corner as the interpreter. </li>\n",
    "</ol>\n",
    "\n",
    "After execution of the above steps all libraries are downloaded and ready to be used. <br>\n",
    "Below all libraries are imported and are given the correct aliasses, which are often used in practice. <br>\n",
    "After running the code cell below your environment is set correctly and you are ready to start coding!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "%cd ..\n",
    "from database.createDB import Database\n",
    "\n",
    "db = Database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3> B: Read data and visualize characteristics </h3>\n",
    "\n",
    "<p> \n",
    "    The start of all Data Science project is understanding the data you are working. <br>\n",
    "    We prepared the data in a SQLite database object named <code>AuctionData.db</code>, which is located next to this file in the source (src) directory. <br>\n",
    "    In the code cell above we established the database connection in the variable <code>db</code>, which can now be used to query the data. <br>\n",
    "    This allows you to write more advanced queries if desired by executing the function: <code>db.execute_query([INSERT QUERY])</code>. <br>\n",
    "    However this is not needed, as we will prepare three queries that extract the data in Pandas Dataframes.<br>\n",
    "    <br>\n",
    "    Gaining insight into the data can be done in several ways, of which we give the most often used below. <br>\n",
    "    As writing code to execute these analyses is not too exciting we already wrote it for you. <br>\n",
    "    Please analyse the output closely to see what it tells about the quality of the underlying data. <br>\n",
    "    Also here, you are encouraged to look further than what we give to you, as the given analyses are in no way extensive or complete. <br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>B1. Retrieve data from the SQLite database into Pandas Dataframes. </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv files\n",
    "auctions = db.execute_query(query=\"SELECT * FROM auctions\")\n",
    "lots = db.execute_query(query=\"SELECT * FROM lots\")\n",
    "bids = db.execute_query(query=\"SELECT * FROM bids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>B2. Print the first records of the dataframes to gain initial insight. </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auctions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lots.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>B3. Retrieve descriptive column statistics of the different database tables. </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auctions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lots.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>B4. Retrieve the statistics of the numeric values of the different database tables.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auctions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lots.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3> C: Use-case examples </h3>\n",
    "<p>\n",
    "Below we will give some boilerplate code to get you started with either one of the examples given above. <br>\n",
    "These examples include:\n",
    "<ol>\n",
    "    <li>Clustering of different Bidding Behavior.</li>\n",
    "    <li>Prediction of sale or no sale.</li>\n",
    "    <li>Automatically setting an \"optimal\" starting bid.</li>\n",
    "</ol>\n",
    "Every example will be given its own subsection, including some pre-written code. <br>\n",
    "This code is however not complete, as we still require you to solve certain key parts of the code. <br>\n",
    "Be aware that you are in no way needed to complete and/or use this code, it is solemnly included to get you up to speed.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h4> C1: Clustering of different bidding behavior. </h4>\n",
    "Our first example implies the clustering of different bidding behaviors found in the data. <br>\n",
    "Through clustering these behaviors we can distinguish certain strategies, which might be useful to manage auctions in the future. <br>\n",
    "Besides being of big descriptive value these clusters can also be used in other frameworks, for example to predict whether or not a product will be sold. <br>\n",
    "<br>\n",
    "\n",
    "We propose to use the KMeans algorithm for clustering, as it is the most often used algorithm in practice. <br>\n",
    "Before being able to cluster the data, we first have to clean and prepare it. <br>\n",
    "To guide you through the preparation, modeling, and evaluation process a list of logical consecutive actions is given below:\n",
    "<ol>\n",
    "    <li> <u><b>Feature Engineering:</b></u> Extract useful characteristics from the dataset. </li>\n",
    "    <li> <u><b>Scale the data:</b></u> Apply scaling to all numeric values to prevent the effect of a <b>distance bias</b>. </li>\n",
    "    <li> <u><b>Find the \"optimal\" number of clusters:</b></u> Use the elbow method in the pre-written code below to find the optimal number of clusters. </li>\n",
    "    <li> <u><b>Cluster the data:</b></u> Using the found optimal number of clusters, cluster the entire dataset. </li>\n",
    "    <li> <u><b>Extract cluster characteristics:</b></u> Retrieve the average values of all parameters per cluster. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> --- C1.1: Feature Engineering --- </h5>\n",
    "<p>\n",
    "First we will extract some interesting statistics about different bidders from the available data. <br>\n",
    "The data points that we propose are the following:\n",
    "<ul>\n",
    "    <li><b>Number of Bids    (NOB)</b> - The Number Of Bids this bidders has placed in the concerning auction lot.</li>\n",
    "    <li><b>Average Bid Price (ABP)</b> - The Average Bid Price of all the bids that the bidders has placed in the concerning auction lot.</li>\n",
    "    <li><b>Highest Bid Price (HBP)</b> - The Highest Bid Price of all the bids that the bidders has placed in the concerning auction lot.</li>\n",
    "    <li><b>Time of Entry     (TOE)</b> - The Time Of Entry, describing at what percentage of total lot duration the bidder placed his first bid.</li>\n",
    "    <li><b>Time of Exit      (TOX)</b> - The Time Of Exit, describing at what percentage of total lot duration the bidder placed his last bid.</li>\n",
    "</ul>\n",
    "We want these values for every bidder in every lot of every auction separately, as different bidders/auctions can result in different behavior. <br>\n",
    "Calculation of these values can be done using the bids data, however some values are needed using a higher aggregation level which we call <b>lot statistics</b>. <br>\n",
    "Your task is to complete the code in the <code>generate_lot_statistic()</code> function, for which a detailed docstring is given in the code. <br>\n",
    "The code in the following <code>generate_bid_statistic()</code> uses this function, so make sure it integrates nicely.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lot_statistic(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate statistics regarding the Lots, describing both the datetime of the fist bid and the actual duration of the lot.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Bids data, describing the bids that are made on specific auction-lot combinations.\n",
    "\n",
    "    Returns:\n",
    "        lot_statistic (pd.DataFrame): Dataframe containing the lot statistics. This needs to contain two columns:\n",
    "            - FirstBid: The datetime of the first bid placed in the lot.\n",
    "            - LotEnding: The datetime at which the lot closes.\n",
    "            - Duration: The time between the first bid and the scheduled time the lot is supposed to end.\n",
    "    \"\"\"\n",
    "                \n",
    "    #TODO: Write the code needed to get from the given input to the desired output as stated in the DOCSTRING\n",
    "    lot_statistic = ...\n",
    "    \n",
    "    return lot_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bid_statistic(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate statistics regarding the bidders, to be used to cluster different bidding behaviors.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Bids data, describing the bids that are made on specific auction-lot combinations.\n",
    "\n",
    "    Returns:\n",
    "        bid_statistic (pd.DataFrame): Dataframe containing the statistics regarding bidders. This needs to contain nine columns:\n",
    "            - AuctionID: The ID reference of the auction in which the bid was placed.\n",
    "            - LotNr: The number reference of the lot in which the bid was placed.\n",
    "            - AccountID: The ID reference of the bidder which placed the bid.\n",
    "            - NOB: The Number Of Bids this bidders has placed in the concerning auction lot.\n",
    "            - ABP: The Average Bid Price of all the bids that the bidders has placed in the concerning auction lot.\n",
    "            - HBP: The Highest Bid Price of all the bids that the bidders has placed in the concerning auction lot.\n",
    "            - TOE: The Time Of Entry, describing at what percentage of total lot duration the bidder placed his first bid.\n",
    "            - TOX: The Time Of Exit, describing at what percentage of total lot duration the bidder placed his last bid.\n",
    "    \"\"\"\n",
    "    # Ensure datetime type in datetime variables\n",
    "    data['BiddingDateTime'] = pd.to_datetime(data['biddingDateTime'], format='%Y-%m-%d %H:%M:%S')\n",
    "    data['ClosingDateTime'] = pd.to_datetime(data['closingDateTime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Group based on accountID, LotNr and AccountID and generate NOB, ABP, HBP, TOE and TOX.\n",
    "    bid_statistic = data.groupby(['auctionID','lotNr','accountID']).agg({'bidPrice': ['count', 'mean', 'max'], \n",
    "                                                                         'biddingDateTime': ['min', 'max']})\n",
    "    bid_statistic.columns = ['NOB', 'ABP', 'HBP', 'TOE', 'TOX']\n",
    "    bid_statistic = bid_statistic.reset_index()\n",
    "\n",
    "    # Convert TOE and TOX to datetimes\n",
    "    bid_statistic['TOE'] = pd.to_datetime(bid_statistic['TOE'], format='%Y-%m-%d %H:%M:%S')\n",
    "    bid_statistic['TOX'] = pd.to_datetime(bid_statistic['TOX'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Generate statistics of the lot and merge with bid statistic\n",
    "    lot_statistic = generate_lot_statistic(data)\n",
    "    bid_statistic = pd.merge(bid_statistic, lot_statistic, on=['auctionID', 'lotNr'], how='left')\n",
    "\n",
    "    # Convert static time characteristics (TOE and TOX) to relative time characteristics using the merged Lot statistics (FirstBid and Duration)\n",
    "    bid_statistic['TOE'] = (bid_statistic['TOE'] - bid_statistic['FirstBid']).apply(lambda x: x/pd.Timedelta('1 minute'))\n",
    "    bid_statistic['TOE'] = bid_statistic['TOE'] / bid_statistic['Duration']\n",
    "\n",
    "    bid_statistic['TOX'] = (bid_statistic['TOX'] - bid_statistic['FirstBid']).apply(lambda x: x/pd.Timedelta('1 minute'))\n",
    "    bid_statistic['TOX'] = bid_statistic['TOX'] / bid_statistic['Duration']\n",
    "    \n",
    "     # Set outliers in normalized TOE or TOX (> 1) to 1\n",
    "    bid_statistic['TOE'] = [x if x <= 1 else 1 for x in bid_statistic['TOE']]\n",
    "    bid_statistic['TOX'] = [x if x <= 1 else 1 for x in bid_statistic['TOX']]\n",
    "\n",
    "    # Remove lot statistics from the data again\n",
    "    bid_statistic.drop(['FirstBid', 'LotEnding'], axis=1, inplace=True)\n",
    "\n",
    "    return bid_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bid_statistic = create_bid_statistic(data=bids)\n",
    "bid_statistic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> --- C1.2: Scale the data --- </h5>\n",
    "<p>\n",
    "To mitigate the potential effect of a distance bias due to values being on different ranges, we need to scale the data. <br>\n",
    "There are several ways to do this, where we will use a simplistic method called <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\">Min-Max Scaling</a>. <br>\n",
    "Applying this method will result in all values being placed between 0 and 1, relative to their original distribution. <br>\n",
    "To implement this we use the Scikit-Learn library, which contains a function called <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\">MinMaxScaler()</a>. <br>\n",
    "Additional information can be found one the documentation page, which can be found following the hyperlink. <br>\n",
    "<br>\n",
    "Below you need to implement the MinMaxScaler of Scikit-Learn to scale the following columns:\n",
    "<ul>\n",
    "    <li><b>Number of Bids    (NOB)</b> - The Number Of Bids this bidders has placed in the concerning auction lot.</li>\n",
    "    <li><b>Average Bid Price (ABP)</b> - The Average Bid Price of all the bids that the bidders has placed in the concerning auction lot.</li>\n",
    "    <li><b>Highest Bid Price (HBP)</b> - The Highest Bid Price of all the bids that the bidders has placed in the concerning auction lot.</li>\n",
    "</ul>\n",
    "Time of Entry (TOE) and Time of Exit (TOX) are left out, as they are already scaled between 0 and 1 due to their inherent relative nature. <br>\n",
    "The function in which you have to create your scaler already has some comments to guide you through the design process. <br>\n",
    "However, feel free to remove these and create your own code in a way you like!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data: pd.DataFrame) -> np.array:\n",
    "    \"\"\"\n",
    "    Apply min-max scaling to the NOB, ABP, HBP and Duration values to get them on the same range between 0 and 1.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Bid statistics dataframe, containing all characteristic values of bidders in a given auction lot.\n",
    "\n",
    "    Returns:\n",
    "        norm_bid_stat (np.array): Normalized bid statistic data, ready to be used for clustering.\n",
    "    \"\"\"\n",
    "    #TODO: Retrieve the NOB, ABP, HBP and Duration value and take them through the MinMaxScaler of the Scikit-Learn library\n",
    "    bid_stat_array = ...\n",
    "    min_max_scaler = ...\n",
    "    norm_bid_stat = ...\n",
    "\n",
    "    # Append the TOE and TOX values to the normalized bid statistics (norm_bid_stat)\n",
    "    norm_bid_stat = np.c_[norm_bid_stat, bid_statistic['TOE'], bid_statistic['TOX']]\n",
    "\n",
    "    return norm_bid_stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = scale_data(data=bid_statistic)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> --- C1.3: Find number of clusters --- </h4>\n",
    "<p>\n",
    "Now we have a normalized data set, we can start the process of designing our <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">KMeans</a> model. <br>\n",
    "A fundamental step for any clustering algorithm is to determine the optimal number of clusters. <br>\n",
    "<br>\n",
    "Too many clusters and the model will overfit, resulting in bad generalization to new data outside the training data. <br>\n",
    "Too little clusters and the model will underfit, resulting in just poor performance. <br>\n",
    "<br>\n",
    "An often used method to determine how many clusters best fits the data is the <a href=\"https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/\">Elbow method</a>.<br>\n",
    "By fitting KMeans algorithms on different amounts of clusters and comparing the <b>Within Cluster Variation (WCV)</b> an \"optimal\" value can be found. <br>\n",
    "The WCV implies the distance between data points belonging to the same cluster, which always decreases when more clusters are formed. <br>\n",
    "By plotting the inertia values for increasing amounts of clusters we intend to find a number of clusters after which this WCV decrease slows down significantly. <br>\n",
    "This points symbolizes an Elbow in an arm, and is seen as the \"optimal\" number of clusters for the underlying data set. <br>\n",
    "<br>\n",
    "Implementation of this method can be done using the following steps:\n",
    "<ol>\n",
    "    <li>Decide a range of cluster amounts that you want to test, we propose a range from 1 to 10.</li>\n",
    "    <li>Initialize a list variable (<b><i>within_cluster_variations</i></b>) that can store all intertia values. </li>\n",
    "    <li>Build a <b>for loop</b> over this range, executing the following steps:\n",
    "        <ol>\n",
    "        <li> Fit the KMeans algorithm on the given number of clusters.</li>\n",
    "        <li> Retrieve the WCV value by calling <code>.inertia_</code> on variable containing the fitted model.</li>\n",
    "        <li> Append the WCV value to the created <b><i>within_cluster_variations</i></b> list variable.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>Plot the <b><i>within_cluster_variations</i></b> list against the range of cluster amounts using the <a href=\"https://matplotlib.org/stable/index.html\">Matplotlib</a> library.</li>\n",
    "    <li>Find the \"optimal\" number of clusters by looking for the \"Elbow\" point in the graph.</li>\n",
    "</ol>\n",
    "Use the steps above to complete the function below that takes in the scaled data and uses it to make the elbow plot for a cluster amount range from 1 to 10. <br>\n",
    "For the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">KMeans</a> implementation use the library of Scikit-Learn, for which we refer to the documentation behind the hyperlink. <br>\n",
    "After creating the Elbow plot set the <b><i>optimal_number_of_clusters</i></b> variable to the \"optimal\" number of clusters found using the Elbow plot.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(scaled_data:np.array) -> None:\n",
    "    \"\"\"\n",
    "    Fit different clustering algorithms with different amounts of clusters to determine the \"optimal\" number of clusters.\n",
    "\n",
    "    Args:\n",
    "        scaled_data (np.array): data that is taken through the MinMax scaling algorithm, ready to be used for clustering.\n",
    "    \"\"\"\n",
    "    #TODO: Fit the Kmeans algorithm with number of clusters varying between 1 and 10 and save the .interia_ value\n",
    "    nr_of_clusters = ...\n",
    "    within_cluster_variations = []\n",
    "    for amount in nr_of_clusters:\n",
    "        ...\n",
    "    \n",
    "    #TODO: Plot the number of clusters against the .intertia_ values found\n",
    "    plt.figure()\n",
    "    plt.plot(...)\n",
    "    plt.xticks(nr_of_clusters)\n",
    "    plt.ylabel('Within Cluster Variation (WCV)')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.title('Elbow method')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_method(scaled_data=scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read the optimal number of clusters from the Elbow plot above.\n",
    "optimal_number_of_clusters = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> --- C1.4: Cluster the data --- </h4>\n",
    "<p>\n",
    "Now we know the optimal number of clusters we can fit our final model used to place inferences on the actual data. <br>\n",
    "In a similar manner as in the previously created <code>elbow_method()</code>, create a variable containing a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">KMeans</a> instance. <br>\n",
    "Upon creation of this instance, set the number of clusters (<b><i>n_clusters</i></b>) to the variable containing the \"optimal\" number of clusters. <br>\n",
    "After fitting the KMeans model to the data the labels can be extracted by calling the <code>.labels_</code> function on the variable containing the model. <br>\n",
    "Please extract these labels and append them to the <b><i>bid_statistic</i></b> as a column names <i>cluster</i>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initialize kmeans model using the optimal number of clusters\n",
    "kmeans = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add cluster labels to the bid_statistics\n",
    "bid_statistic['cluster'] = ...\n",
    "bid_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> --- C1.5: Extract cluster characteristics --- </h4>\n",
    "<p>\n",
    "So far we prepared the data, found the \"optimal\" number of clusters and used a trained model to assign data points to clusters. <br>\n",
    "These cluster assignments on their own do not have that much descriptive value, as there is no insight into what these clusters imply. <br>\n",
    "Gaining this insight can be done through extraction of cluster characteristics, which will be your next task. <br>\n",
    "<br>\n",
    "One way to do this is using the Pandas Groupby function to group on cluster and calculate the mean values for all variables. <br>\n",
    "But of course again only the output counts, so if you know another way feel free to do it your way. <br>\n",
    "Simply put, what we desire to create are the following values for all clusters separately:\n",
    "<ol>\n",
    "    <li> Average NOB (Number of Bids) </li>\n",
    "    <li> Average ABP (Average Bid Price) </li>\n",
    "    <li> Average HBP (Highest Bid Price) </li>\n",
    "    <li> Average TOE (Time of Entry) </li>\n",
    "    <li> Average TOX (Time of Exit) </li>\n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cluster_statistics(bid_statistic:np.array) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Use the created bid statistics together with the assigned labels to calculate different cluster statistics.\n",
    "\n",
    "    Args:\n",
    "        bid_statistic (np.array): the calculated statistics of different bidders and their assigned cluster.\n",
    "\n",
    "    Returns:\n",
    "        cluster_statistics (pd.DataFrame): Dataframe containing the means of all values for all clusters separately.\n",
    "    \"\"\"\n",
    "    #TODO: Calculate cluster aggregates using the '.groupby()' and '.agg()' function of the Pandas Library\n",
    "    cluster_statistics = ...\n",
    "\n",
    "    return cluster_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_cluster_statistics(bid_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h4> C2: Predict sale or no-sale </h4>\n",
    "<p>\n",
    "After clustering (which is unsupervised learning) we are now going to apply supervised learning in the form of forecasting. <br>\n",
    "Several aspects of the process can be forecasted, but we decided to build a model which will predict if a product sells or not. <br>\n",
    "As in most cases, we are currently not sure which model will fit the data best, so we will try out two different models. <br>\n",
    "For this purpose we chose the <code>LogisticRegression()</code> and <code>RandomForestClassifier()</code>. <br>\n",
    "To get you started we listed some steps that you can follow to create, train and use these models on the auction data. <br>\n",
    "Feel free to explore your own ways, but if needed you can always look back at these steps:\n",
    "<ol>\n",
    "    <li> <u><b>Preprocess the data:</b></u> Merge and preprocess the auction and lots data. </li>\n",
    "    <li> <u><b>Create train and test set:</b></u> Create a 70/30 <b>stratified</b> training/test set of the lots dataset. </li>\n",
    "    <li> <u><b>Initialize and train models:</b></u> Initialize and train both the <code>LogisticRegression()</code> and <code>RandomForestClassifier()</code>. </li>\n",
    "    <li> <u><b>Evaluate model performances:</b></u> Use the model on the test set and calculate <b>accuracy</b> and a <b>confusion matrix</b>. </li>\n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> --- C2.1: Preprocess the data --- </h4>\n",
    "<p>\n",
    "The starting point of every Data Science job is to get your data ready. <br>\n",
    "For the purpose of predicting whether a lot will sell, we expect only to need the <b><i>lots</i></b> and <b><i>auction</i></b> data. <br>\n",
    "Our first step will be to select the desired columns that we are going to use to make the prediction. <br>\n",
    "We already made a pre-selection for you, as we can imagine it is hard to oversee a data set in the amount of time that you have been given. <br>\n",
    "However, we might have missed some interesting values, so make sure to look through the data yourself if time permits.<br>\n",
    "<br>\n",
    "After column selection we will merge both data sets, taking the <b><i>lots</i></b> data as the main reference upon which the auction data will merge. <br>\n",
    "After merging, three preprocessing steps still need to happen:\n",
    "<ul>\n",
    "    <li> Transform the datetime columns (<i>auctionStart</i> and <i>auctionEnd</i>) into numerical value <i>auctionDuration</i>. </li>\n",
    "    <li> Apply one-hot encoding to categorical variables using Pandas <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html\">get_dummies()</a> function. </li>\n",
    "    <li> Scale the numerical values in the merged dataset using the Scikit-Learn <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\">MinMaxScaler()</a>. </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve desired columns from lots and auction datasets\n",
    "Lots_Sales_Pred = lots[['auctionID', 'lotNr', 'numberOfItems', 'estimatedValue', 'startingBid', 'reserveBid', 'sold']].copy()\n",
    "Auctions_merge = auctions[['id', 'auctionStart', 'auctionEnd', 'branchCategory']].copy()\n",
    "\n",
    "# Combine data into a single dataset\n",
    "Lots_Sales_Pred = pd.merge(Lots_Sales_Pred, Auctions_merge, left_on=['auctionID'], right_on=['id'], how='left')\n",
    "\n",
    "Lots_Sales_Pred = Lots_Sales_Pred.dropna()  \n",
    "Lots_Sales_Pred.drop('id', axis=1, inplace=True)\n",
    "Lots_Sales_Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform auctionStart and auctionEnd into a single column depicting the duration of the auction in hours\n",
    "Lots_Sales_Pred[['auctionStart', 'auctionEnd']] = Lots_Sales_Pred[['auctionStart', 'auctionEnd']].apply(pd.to_datetime, errors='coerce') \n",
    "Lots_Sales_Pred['auctionDuration'] = (Lots_Sales_Pred['auctionEnd'] - Lots_Sales_Pred['auctionStart']).apply(lambda x: abs(x/pd.Timedelta('1 hour')))\n",
    "Lots_Sales_Pred.drop(['auctionStart', 'auctionEnd'], axis=1, inplace=True)\n",
    "\n",
    "Lots_Sales_Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: One-hot-encode the categorical variable 'branchCategory' using the get_dummies function of the Pandas library\n",
    "OneHotEncoded = ...\n",
    "Lots_Sales_Pred = pd.concat([Lots_Sales_Pred, OneHotEncoded ],axis=1)\n",
    "Lots_Sales_Pred.drop(['branchCategory'], axis=1, inplace=True)\n",
    "\n",
    "Lots_Sales_Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Apply the MinMaxScaler() of Scikit-Learn on the numerical values of the dataframe\n",
    "scl = ...\n",
    "Lots_Sales_Pred[['numberOfItems', 'estimatedValue', 'startingBid', 'reserveBid', 'auctionDuration']] = ...\n",
    "\n",
    "Lots_Sales_Pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> --- C2.2: Create train and test set --- </h4>\n",
    "<p>\n",
    "Before creating the train/test split, we first have to remove unwanted columns (<i>auctionID and lotNr</i>) and have to extract the prediction label (<i>sold</i>) from the data set. <br>\n",
    "Creation of a prediction model is done using a training dataset, containing a wide selection of data points that represent the entire data set. <br>\n",
    "However, to have a fair evaluation of the model performance we need a test set that the model has not seen before during training. <br>\n",
    "To do this we can use the Scikit-Learn <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">train_test_split()</a> function. <br>\n",
    "Two important hyperparameters in this function are:\n",
    "<ol>\n",
    "    <li> the <i>test_size</i> which we will set to 30% (0.3). </li>\n",
    "    <li> <i>stratify</i> which we will set to the <b><i>y_sale</i></b> variable.</li>\n",
    "</ol>\n",
    "The latter (<i>stratify</i>) ensures that the train and test set contain equal occurrences of the labels found in the sold column. <br>\n",
    "Doing this will prevent a false distribution of the labels present in these sets to affect model performance.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Split the data in indepent (X) and dependent (y) variables\n",
    "X_sale = ...\n",
    "y_sale = ...\n",
    "\n",
    "#TODO: Create a train test split using the Scikit-Learn library\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "print(f'Created a training set containing {len(X_train)} records and a test set containing {len(X_test)} records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> --- C2.3: Initialize and train models --- </h4>\n",
    "<p>\n",
    "After preprocessing the data and creation of our train and test sets, we can start with desiging the prediction models. <br>\n",
    "As previously mentioned, we propose to test both the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">LogisticRegression()</a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">RandomForestClassifier()</a> using our dear friend the Scikit-Learn library. <br>\n",
    "<br>\n",
    "Initialization of these models allows you as a data scientist to set different hyperparameters. <br>\n",
    "If you go on and deploy your models in a real-world scenario, it is needed to tweak these values to be aligned with the underlying task. <br>\n",
    "In the interest of time, we do not suspect this to be needed for now. <br>\n",
    "Please create both a <b><i>logRes</i></b> variable containing the Logistic Regression model and a <b><i>rf</i></b> variable containing the Random Forest Classifier below. <br>\n",
    "<br>\n",
    "After initializing the models we will train the models on our training data which we created earlier. <br>\n",
    "Training a model is made simple by the Scikit-Learn library, as you only have to call the function <code>.fit(X_train, y_train)</code> on the model variable. <br>\n",
    "The way in which this needs to be implemented can be found in the documentation of both models, which can be found following the hyperlinks mentioned above.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRes = ...\n",
    "rf = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fit the Logistic Regression (logRes) model to the training data\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fit the Random Forest Classifier (rf) model to the training data\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> --- C2.4: Evaluate model performances --- </h4>\n",
    "<p>\n",
    "The final step in model development is evaluating the performance of our model. <br>\n",
    "This evaluation might help us in determining which model performs best and thus which model should be used for our problem. <br>\n",
    "For evaluation we will use the models we trained to predict the labels of the test data set by calling the function <code>.predict(X_test, y_test)</code>. <br>\n",
    "Thereafter we will compare the predicted labels with the actual labels of this data set to determine the model performance. <br>\n",
    "<br>\n",
    "A simple starting point in model evaluation is the accuracy score, which implies the fraction of correctly classified data points. <br>\n",
    "As we have done throughout this use case we will use the Scikit-Learn library, more specifically the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\">accuracy_score(y_true, y_pred)</a> function. <br>\n",
    "By giving the true labels (<i>y_true</i>) of the test data set, and the predicted labels (<i>y_pred</i>) by our model to this function it will return the attained accuracy. <br>\n",
    "<br>\n",
    "To gain further insight into this performance an often used method is the <b>Confusion matrix</b>. <br>\n",
    "A Confusion matrix simply displays the predicted values compared to the actual values for all labels separately. <br>\n",
    "Again we will use the Scikit-Learn library for this, using both the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\">confusion_matrix()</a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\">ConfusionMatrixDisplay()</a>. <br>\n",
    "The former is used to create the actual matrix, where the latter is used to visualize the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the trained models to predict the labels of the test data\n",
    "y_pred_logRes = ...\n",
    "y_pred_rf = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Calculate the accuracy score for both the Logistic Regression and Random Forest\n",
    "logRes_accuracy = ...\n",
    "rf_accuracy = ...\n",
    "\n",
    "print(f'Accuracy Logistic Regression: {round(logRes_accuracy,3)}')\n",
    "print(f'Accuracy Random Forest Classification: {round(rf_accuracy,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Construct and display the confusion matrix using the Scikit-Learn library for the Logistic Regression\n",
    "print(f'Confusion Matrix: Logistic Regression')\n",
    "cm_logRes = ...\n",
    "disp = ...\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Construct and display the confusion matrix using the Scikit-Learn library for the Random Forest\n",
    "print(f'Confusion Matrix: Random Forest Classifier')\n",
    "cm_rf = ...\n",
    "disp = ...\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h4> C3: Automatically setting an \"optimal\" starting bid. </h4>\n",
    "<p>\n",
    "Our final example project is automatically setting a starting bid, using our previously created prediction model. <br>\n",
    "In doing this we make a massive assumption, namely that the higher the starting bid the better the auction outcome. <br>\n",
    "Based on this assumption, we can simply iterate over a range of values and see what the highest value is for which the product is still sold. <br>\n",
    "As this operation does not include any fancy libraries and is just plain Python we let you figure out this one yourself. <br>\n",
    "All the needed steps are already discussed in the parts above, so good luck!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_starting_bid(minimum_starting_bid:int, maximum_starting_bid:int) -> int:\n",
    "    \"\"\"\n",
    "    Iterate over the range from the minimum to maximum starting bid. Return the highest value that is predicted to be sold.\n",
    "\n",
    "    Args:\n",
    "        minimum_starting_bid (int): The minimal acceptable value for the starting bid.\n",
    "        maximum_starting_bid (int): The maximal acceptable value for the starting bid.\n",
    "\n",
    "    Returns:\n",
    "        int: The maximum starting bid that still results in a 'sold' prediction.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<h2> Create your own ideas! </h2>\n",
    "<p>\n",
    "You either have worked through all the examples above or you think your idea is better, either way we like it! <br>\n",
    "The code cells below can be used to build your own code and test your own ideas. <br>\n",
    "We are looking forward to what you come up with, bonus points for creativity which leads to a 'satisfied customer'. <br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3915fdc67f82b21c51fb61cdafb1dc46991e50956ccfaa0216548fc011608fa4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
